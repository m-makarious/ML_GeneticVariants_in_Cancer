{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Medicine: Looking at Genetic Variants in the context of Cancer\n",
    "## Kaggle Competition \n",
    "\n",
    "- **By:** Mary B. Makarious\n",
    "- **Project:** FAES BIOF509 - Applied ML Final Project \n",
    "- **Date Written:** 04.05.2019 \n",
    "- **Last Updated:** 15.05.2019 \n",
    "\n",
    "**Description:** This notebook is intended for the ML final project for the NIH FAES BIOF509 - Applied ML course. The data used here is from a Kaggle dataset found here: https://www.kaggle.com/c/msk-redefining-cancer-treatment/data and this project is intended to explore different machine learning algorithms on cancer data. \"Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers)... Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually annotated into 9 different categories.\"\n",
    "\n",
    "**Data:** 2 different kinds of files:\n",
    "- Contains information about the genetic variants: `training_variants.csv` and `test_variants.csv`\n",
    "- Contains the clinical evidence (in text form) that was used to manually classify the variants `training_text.csv` and `test_text.csv`\n",
    "\n",
    "There is a class target feature corresponding to 1 of the 9 categories variants will be classified as in the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing + Setting Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import os #To set wd \n",
    "import re #To use regex\n",
    "import string \n",
    "import math\n",
    "import pandas as pd #Loading and manipulating data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# For NLP Preprocessing\n",
    "import nltk\n",
    "# nltk.download('stopwords') #Download stopwords if not already in environment\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import utils\n",
    "import tqdm\n",
    "import keras\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For NN\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# Data is too large to host on GitHub, so accessing it locally for now\n",
    "os.chdir(\"/Users/makariousmb/Desktop/FAES_ML_FinalProject/msk-redefining-cancer-treatment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading + Formatting + Merging the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in all the data, formatting, and generating Pandas dataframes\n",
    "\n",
    "# Read in the training variants file, which contains information on genetic variants \n",
    "train_variant_df = pd.read_csv(\"training_variants.csv\")\n",
    "#train_variant_df.head()\n",
    "\n",
    "# Read in the test variants file (also containing information on genetic variants)\n",
    "test_variant_df = pd.read_csv(\"stage2_test_variants.csv\")\n",
    "#test_variant_df.head()\n",
    "\n",
    "# Read in the training text (clinical evidence used to manually classify the variants)\n",
    "train_text_df = pd.read_csv(\"training_text.csv\", sep = \"\\|\\|\", engine = \"python\", header = None, skiprows = 1, names = [\"ID\",\"Text\"])\n",
    "#train_text.head()\n",
    "\n",
    "# Read in the test text (also clinical evidence used to manually classify the variants)\n",
    "test_text_df = pd.read_csv(\"stage2_test_text.csv\", sep = \"\\|\\|\", engine = \"python\", header = None, skiprows = 1, names = [\"ID\", \"Text\"])\n",
    "#test_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of training variants available is: 3321\n"
     ]
    }
   ],
   "source": [
    "# Merge the training data together\n",
    "\n",
    "# Merge\n",
    "train_data = pd.merge(train_variant_df, train_text_df, how = \"left\", on = \"ID\")\n",
    "#train_data.head()\n",
    "\n",
    "# Set up the data: Remove \"Class\" since this is what we will try to predict later \n",
    "train_y = train_data[\"Class\"].values #Return numpy representation of this dataframe for later \n",
    "train_x = train_data.drop(\"Class\", axis = 1) #axis=1 to drop actual column and not index value \n",
    "\n",
    "# Find the number of training variants \n",
    "training_size = len(train_x)\n",
    "print(\"The total number of training variants available is: %d\" % (training_size)) #3321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of available variants to test is: 986\n"
     ]
    }
   ],
   "source": [
    "# Merge the test data together\n",
    "\n",
    "# Merge \n",
    "test_x = pd.merge(test_variant_df, test_text_df, how  = \"left\", on = \"ID\")\n",
    "#train_data.head()\n",
    "\n",
    "# Find the number of test variants\n",
    "testing_size = len(test_x)\n",
    "print(\"The total number of available variants to test is: %d\" % (testing_size)) #986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID    Gene               Variant  \\\n",
       "0  0  FAM58A  Truncating Mutations   \n",
       "1  1     CBL                 W802*   \n",
       "2  2     CBL                 Q249E   \n",
       "3  3     CBL                 N454D   \n",
       "4  4     CBL                 L399V   \n",
       "\n",
       "                                                Text  \n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
       "1   Abstract Background  Non-small cell lung canc...  \n",
       "2   Abstract Background  Non-small cell lung canc...  \n",
       "3  Recent evidence has demonstrated that acquired...  \n",
       "4  Oncogenic mutations in the monomeric Casitas B...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all the data together \n",
    "\n",
    "# Index the values for later \n",
    "testing_index = test_x[\"ID\"].values\n",
    "\n",
    "# Merge\n",
    "all_the_data = np.concatenate((train_x, test_x))\n",
    "\n",
    "# Convert back to a pandas df\n",
    "all_the_data = pd.DataFrame(all_the_data)\n",
    "\n",
    "# Force column names\n",
    "all_the_data.columns = [\"ID\", \"Gene\", \"Variant\", \"Text\"]\n",
    "all_the_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with NLP\n",
    "\n",
    "The text needs to be converted to vectors so that ML can be applied. This is done using NLP.\n",
    "Instead of reinventing the wheel, I am using a modified version of the NLP script found here: https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to clean and generate sentences\n",
    "\n",
    "# Create a function that will clean the text\n",
    "    # This will remove any odd characters\n",
    "def textClean(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split() # Force the text to be lowercase \n",
    "    stops = set(stopwords.words(\"english\")) # Use the English stopwords (ex: I, me, my, etc.)\n",
    "    text = [word for word in text if not word in stops] # Join each word one by one as long as they are not the common English \"stopwords\"   \n",
    "    text = \" \".join(text) # Append\n",
    "    return(text)\n",
    "\n",
    "# Create a function that cleans the sentences \n",
    "    # Take in the text, processes it using the function above, and then generates a translation table with mapped characters\n",
    "    # This function takes in strings as inputs, so if need be force cast the column datatype to string for this to work\n",
    "def cleanup(text):\n",
    "    text = textClean(text) \n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) #.maketrans() : returns a translation table that maps each character in the intabstring into the character at the same position in the outtab string\n",
    "    return text\n",
    "\n",
    "# Create a function that will take in data and generate sentences that are labeled\n",
    "    # This will take in a dataset, generate a list of sentences, and split accordingly\n",
    "def constructLabeledSentences(data):\n",
    "    sentences=[]\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(TaggedDocument(utils.to_unicode(row).split(), [\"Text\" + \"_%s\" % str(index)])) # Index each sentence to do ML later\n",
    "    return sentences\n",
    "\n",
    "# TaggedDocument: TaggedDocument(namedtuple('TaggedDocument', 'words tags'))\n",
    "    # StackOverflow Ex: https://stackoverflow.com/questions/45125798/how-to-use-taggeddocument-in-gensim\n",
    "    # LabeledSentences is now deprecated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    cyclindependent kinases cdks regulate variety ...\n",
      "1    abstract background nonsmall cell lung cancer ...\n",
      "2    abstract background nonsmall cell lung cancer ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Clean up all the text\n",
    "\n",
    "# Convert the text column to a string type to be applied with the cleanup function \n",
    "all_the_data[\"Text\"] = all_the_data[\"Text\"].astype('str')\n",
    "\n",
    "# Clean up the text\n",
    "all_cleaned_text = all_the_data[\"Text\"].apply(cleanup)\n",
    "print(all_cleaned_text[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentences with the now cleaned text\n",
    "labeled_sentences = constructLabeledSentences(all_cleaned_text)\n",
    "\n",
    "# View the first sentence (index at the end)\n",
    "#print(labeled_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep and Feature Extraction\n",
    "Use Doc2Vec/Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Implementation Example: https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "# Doc2Vec Documentation: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "# \"Right Number\" of Epochs? : https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n",
    "\n",
    "## Time Spent on this Chunk\n",
    "# 5 Epochs = 10 min\n",
    "\n",
    "text_input_dimensions = 300 \n",
    "\n",
    "max_epochs = 5 # One Epoch is when the whole dataset is passed forward and backward through the neural network once\n",
    "alpha = 0.025 # alpha: the initial learning rate\n",
    "\n",
    "model = Doc2Vec(vector_size = text_input_dimensions, # Dimensionality of the feature vectors\n",
    "               alpha = alpha,\n",
    "               min_alpha = 0.00025, \n",
    "               min_count = 1, # Ignores all words with total frequency lower than 1\n",
    "               dm = 1,  #dm: =0: Bag of Words, =1: Distributed memory to preserve word order \n",
    "               window = 5, # The maximum distance between the current and predicted word within a sentence\n",
    "               sample = 1e-4, # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "               negative = 5,\n",
    "               workers = 4)\n",
    "\n",
    "model.build_vocab(labeled_sentences)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"Iteration {0}\".format(epoch))\n",
    "    model.train(labeled_sentences,\n",
    "               total_examples = model.corpus_count,\n",
    "               epochs = max_epochs) \n",
    "    \n",
    "# Decrease the learning rate to prevent overfitting\n",
    "model.alpha -= 0.0002\n",
    "\n",
    "# Fix the learning rate by having no decay\n",
    "model.min_alpha = model.alpha\n",
    "\n",
    "# Save the model to use later \n",
    "model.save(\"d2v.pm.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41466102  0.83406866 -1.56070006 -1.9632467   1.77578938 -0.48561618\n",
      "  0.10655449  0.28365999 -2.33253694  1.84953225]\n"
     ]
    }
   ],
   "source": [
    "# Make the text into features found in training and test sets\n",
    "# Note: Python indexes at 0, not 1\n",
    "\n",
    "# np.zeros: Return a new array of given shape and type, filled with zeros\n",
    "train_text_set = np.zeros((training_size, text_input_dimensions))\n",
    "test_text_set = np.zeros((testing_size, text_input_dimensions))\n",
    "\n",
    "# Match up the trained vectors to the training set \n",
    "i = 0\n",
    "for i in range(training_size):\n",
    "    train_text_set[i] = model.docvecs['Text_'+str(i)] # .docvecs: Holds all trained vectors for the 'document tags' seen during training\n",
    "\n",
    "# Match up \n",
    "j = 0\n",
    "for i in range(training_size, training_size + testing_size):\n",
    "    test_text_set[j] = model.docvecs['Text_'+str(i)]\n",
    "    j=+1\n",
    "\n",
    "# Print results \n",
    "print(test_text_set[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizing genes and variants \n",
    "    # Using singular-value decomp. (SVD) to have more manageable features\n",
    "    \n",
    "# Readings:\n",
    "    # SVD: https://medium.com/the-andela-way/foundations-of-machine-learning-singular-value-decomposition-svd-162ac796c27d\n",
    "    # get_dummies function: http://fastml.com/how-to-use-pd-dot-get-dummies-with-the-test-set/\n",
    "        # get_dummies(): Convert categorical variable into dummy/indicator variables\n",
    "    # \"One Hot\" Encoding: https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "        # Seems to be better than label encoding so as not to rely on categorical placement/incorrect weights\n",
    "        # \"This is why we use one hot encoder to perform 'binarization' of the category and include it as a feature to train the model.\"\n",
    "    # Difference between LabelEncoder and pd.get_dummies(): https://stackoverflow.com/questions/38413579/what-is-the-difference-between-sklearn-labelencoder-and-pd-get-dummies\n",
    "        # LabelEncoder(): \"condenses\" the information by changing things to integers\n",
    "        # get_dummies(): \"expands\" the dimensions allowing (possibly) more convenient access\n",
    "\n",
    "# Set gene dimensions\n",
    "gene_input_dimensions = 25\n",
    "\n",
    "# Made the SVD model to truncate the gene and variant information later \n",
    "# SVD: Factorize into 3 matrices, then decompose into lower rank matrices without losing much of the important data\n",
    "svd = TruncatedSVD(n_components = 25,\n",
    "                   n_iter = gene_input_dimensions,\n",
    "                   random_state = 12)\n",
    "\n",
    "# Convert the categorical gene information by expanding it into different columns using one-hot encoding \n",
    "# Decompose with SVD \n",
    "one_hot_gene = pd.get_dummies(all_the_data[\"Gene\"])\n",
    "svd_one_hot_gene = svd.fit_transform(one_hot_gene.values)\n",
    "\n",
    "# Convert the categorical variant information by expanding it into different columns using one-hot encoding\n",
    "# Decompose with SVD \n",
    "one_hot_variant = pd.get_dummies(all_the_data[\"Variant\"])\n",
    "svd_one_hot_variant = svd.fit_transform(one_hot_variant.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Output the class encoding \n",
    "# LabelEncoder():\n",
    "    # Can be used to normalize labels\n",
    "    # Can be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels \n",
    "    \n",
    "# Consolidate the dataset using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the training set\n",
    "label_encoder.fit(train_y)\n",
    "\n",
    "# Convert the array of labeled data to one-hot encoded vector\n",
    "# Apply to pandas dataframe \n",
    "encoded_training_y = np_utils.to_categorical((label_encoder.transform(train_y)))\n",
    "print(encoded_training_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape is:  (3321, 350)\n",
      "Test set shape is:  (986, 350)\n",
      "\n",
      " Training set example rows:\n",
      "[-3.83415240e-23 -8.12746096e-19  3.25283397e-22 -3.52950754e-22\n",
      " -1.91577362e-22  3.54676519e-25  6.61614239e-25 -2.40019986e-27\n",
      "  3.49275715e-28  7.31181426e-30]\n",
      "\n",
      " Test set example rows:\n",
      "[ 2.69770557e-32 -2.77827596e-27 -3.63497563e-27 -3.12866459e-27\n",
      "  7.75549748e-27 -1.13244416e-26 -1.64947289e-26 -8.94629705e-26\n",
      " -3.75320953e-23 -1.05563464e-21]\n"
     ]
    }
   ],
   "source": [
    "# View consolidated training and testing sets\n",
    "# np.hstack(): Used to stack the sequence of input array horizontally (so, column-wise) to make a single array\n",
    "\n",
    "training_set = np.hstack((svd_one_hot_gene[:training_size], svd_one_hot_variant[:training_size], train_text_set))\n",
    "testing_set = np.hstack((svd_one_hot_gene[training_size:], svd_one_hot_variant[training_size:], test_text_set))\n",
    "\n",
    "# Dataset Attribute Updates \n",
    "print(\"Training set shape is: \", training_set.shape) \n",
    "print(\"Test set shape is: \", testing_set.shape) \n",
    "\n",
    "print(\"\\n Training set example rows:\")\n",
    "print(training_set[0][:10])\n",
    "\n",
    "print(\"\\n Test set example rows:\")\n",
    "print(testing_set[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading\n",
    "    # NN: https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "    # Train/Test Split: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    # NN Example: https://bit.ly/2WEY2Hs\n",
    "    # Simpler breakdown of components needed for NN: https://towardsdatascience.com/understanding-neural-networks-what-how-and-why-18ec703ebd31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (350, 2656)\n",
      "X_validation:  (350, 665)\n",
      "Y_train:  (9, 2656)\n",
      "Y_validation:  (9, 665)\n",
      "X_test:  (350, 986)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into the training and validation sets \n",
    "# Training set\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(training_set, encoded_training_y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "# Transpose the training set\n",
    "X_train, X_validation, Y_train, Y_validation = X_train.T, X_validation.T, Y_train.T, Y_validation.T\n",
    "\n",
    "# Transpose the testing set \n",
    "X_test = testing_set.T\n",
    "\n",
    "# View the dataset shapes to create placeholders later for TensorFlow...\n",
    "\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_validation: \", X_validation.shape)\n",
    "print(\"Y_train: \", Y_train.shape)\n",
    "print(\"Y_validation: \", Y_validation.shape)\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to initialize the parameters that TensorFlow will use to build a NN\n",
    "    # \"Tensor\" dictionaries containing weights and biases for each layer\n",
    "    # Keeping things simple(r), will only program 4 layers for now\n",
    "    \n",
    "def initialize_tf_parameters():\n",
    "    parameters = {}\n",
    "    tf.set_random_seed(1) # keep things the same across the entire NN \n",
    "        # Layer 1\n",
    "    weights_1 = tf.get_variable(\"weights_1\", [350, X_train.shape[0]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    bias_1 = tf.get_variable(\"bias_1\", [350, 1], initializer = tf.zeros_initializer())\n",
    "        # Layer 2\n",
    "    weights_2 = tf.get_variable(\"weights_2\", [350, 350], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    bias_2 = tf.get_variable(\"bias_2\", [350, 1], initializer = tf.zeros_initializer())\n",
    "        # Layer 3 \n",
    "    weights_3 = tf.get_variable(\"weights_3\", [100, 350], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    bias_3 = tf.get_variable(\"bias_3\", [100, 1], initializer = tf.zeros_initializer())\n",
    "        # Layer 4 \n",
    "    weights_4 = tf.get_variable(\"weights_4\", [9, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    bias_4 = tf.get_variable(\"bias_4\", [9, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\"weights_1\": weights_1,\n",
    "                  \"bias_1\": bias_1,\n",
    "                  \"weights_2\": weights_2,\n",
    "                  \"bias_2\": bias_2,\n",
    "                  \"weights_3\": weights_3,\n",
    "                  \"bias_3\": bias_3,\n",
    "                  \"weights_4\": weights_4,\n",
    "                  \"bias_4\": bias_4}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Create a function to create the placeholders needed for TensorFlow \n",
    "    # input_x: Dimensions of the input (scalar)\n",
    "    # input_y: # of classes (there are 9 mutational classes and Python indexes from 0, so you need 8 as the input here)\n",
    "    # X: Placeholder in floats with the shape of input_x \n",
    "    # Y: Placeholder in floats with the shape of input_y \n",
    "    \n",
    "def create_tf_placeholders(input_x, input_y):\n",
    "    X = tf.placeholder(tf.float32, shape = (input_x, None), name = \"X\") \n",
    "    Y = tf.placeholder(tf.float32, shape = (input_y, None), name = \"Y\")\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"forward propogation\" function for the NN\n",
    "## Reading\n",
    "    # Consider \"Activation Functions\": https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76 (???)\n",
    "    # Rectified Linear Unit (ReLU): https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/weights_\n",
    "    # Softmax: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\n",
    "        # Softmax assumes that each example is a member of exactly one class (which is what we need, each mutation will be sorted into 1 of 9 classes)\n",
    "    # Keeping probabilites: https://stats.stackexchange.com/questions/324914/value-of-the-keep-probability-when-calculating-loss-with-dropout\n",
    "        # Meant to calculate loss of dropping out things \n",
    "        # Will calculate loss later\n",
    "        \n",
    "# Input and output for forward-feeding function \n",
    "    # Combination of linear and ReLU to generate a softmax \n",
    "    # Takes in the placeholder generated in the function prior \n",
    "    # Takes in the parameters of weights and biases for each of the layers\n",
    "    # Keeps probablities for calculating the loss of dropouts \n",
    "    # Returns the linear transformation of the 4 layer NN (Z_linear_transform_3)\n",
    "    \n",
    "def forward_propagation(X, parameters, keep_probability_1, keep_probability_2):\n",
    "        # Parameters from dictionary of parameters \n",
    "    weights_1 = parameters[\"weights_1\"]\n",
    "    bias_1 = parameters[\"bias_1\"]\n",
    "    weights_2 = parameters[\"weights_2\"]\n",
    "    bias_2 = parameters[\"bias_2\"]\n",
    "    weights_3 = parameters[\"weights_3\"]\n",
    "    bias_3 = parameters[\"bias_3\"]\n",
    "    weights_4 = parameters[\"weights_4\"]\n",
    "    bias_4 = parameters[\"bias_4\"]\n",
    "    \n",
    "        # Forward feeding the NN\n",
    "    Z_linear_transform_1 = tf.matmul(weights_1, X) + bias_1  # Z1 = np.dot(W1, X) + b1\n",
    "    A_post_activation_1 = tf.nn.relu(Z_linear_transform_1)  # A1 = relu(Z1)\n",
    "    A_post_activation_1 = tf.nn.dropout(A_post_activation_1, keep_probability_1)  # add dropout\n",
    "    Z_linear_transform_2 = tf.matmul(weights_2, A_post_activation_1) + bias_2  # Z2 = np.dot(W2, a1) + b2\n",
    "    A_post_activation_2 = tf.nn.relu(Z_linear_transform_2)  # A2 = relu(Z2)\n",
    "    A_post_activation_2 = tf.nn.dropout(A_post_activation_2, keep_probability_2)  # add dropout\n",
    "    Z_linear_transform_3 = tf.matmul(weights_3, A_post_activation_2) + bias_3  # Z3 = np.dot(W3,Z2) + b3\n",
    "    A_post_activation_3 = tf.nn.relu(Z_linear_transform_3)\n",
    "    Z_linear_transform_4 = tf.matmul(weights_4, A_post_activation_3) + bias_4\n",
    "\n",
    "    return Z_linear_transform_4\n",
    "\n",
    "# Create a function that will calculate loss\n",
    "    # Loss functions: https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "    # \"... must faithfully distill all aspects of the model down into a single number in such a way that improvements in that number are a sign of a better model\"\n",
    "    # TensorFlow will do this for you: https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2\n",
    "        \n",
    "# Input and output for the cost function \n",
    "    # Z_linear_transform of the last linear unit given from the forward propogation\n",
    "    # Placeholder that is the same shape of the last linear unit \n",
    "    # Returns TensorFlow function of cost \n",
    "\n",
    "def calculate_cost(Z_linear_transform_4, Y):\n",
    "    # TensforFlow has a function that will do this for you\n",
    "    # You just need to clean up the data a little bit by transposing it \n",
    "    logits = tf.transpose(Z_linear_transform_4) # Unscaled log probabilities\n",
    "    labels = tf.transpose(Y) # Each vector along the class dimension should hold a valid probability distribution\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini Batching\n",
    "    # Why mini-batches are good: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
    "    # Mini batches vs. Stochastic Loss: https://medium.com/coinmonks/stochastic-vs-mini-batch-training-in-machine-learning-using-tensorflow-and-python-7f9709143ee2\n",
    "\n",
    "## Create a function that will generate mini batches and shuffle them around \n",
    "# Input and output for mini batching\n",
    "    # X: The actual input data\n",
    "    # Y: The labels (this has to be of shape 1...?)\n",
    "    # Specifying the size of the mini batches (integer)\n",
    "    # Specifying the seed so it can stay consistent throughout \n",
    "    # Will return/output \n",
    "\n",
    "def generate_mini_batches_random(X, Y, size_of_mini_batch, seed = 0):\n",
    "    mini_batches = [] # Initialize list \n",
    "    np.random.seed(seed) # Keep the seed the same to keep it consistent \n",
    "    num_train_examples = X.shape[1]  # Number of training examples\n",
    "\n",
    "    ## Need to shuffle around the batches randomly \n",
    "    # Part 1: Randomize/Shuffle both X and Y\n",
    "    permutation = list(np.random.permutation(num_train_examples))\n",
    "    X_randomized = X[:, permutation]\n",
    "    Y_randomized = Y[:, permutation].reshape((Y.shape[0], num_train_examples))\n",
    "\n",
    "    # Part 2: Partitioning your sets (X_randomized, Y_randomized)\n",
    "        # Everything BUT your last case which will be sorted out later \n",
    "    num_complete_minibatches = math.floor(num_train_examples / size_of_mini_batch) \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X_randomized[:, k * size_of_mini_batch: k * size_of_mini_batch + size_of_mini_batch]\n",
    "        mini_batch_Y = Y_randomized[:, k * size_of_mini_batch: k * size_of_mini_batch + size_of_mini_batch]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case\n",
    "    if num_train_examples % size_of_mini_batch != 0:\n",
    "        mini_batch_X = X_randomized[:, num_complete_minibatches * size_of_mini_batch: num_train_examples]\n",
    "        mini_batch_Y = Y_randomized[:, num_complete_minibatches * size_of_mini_batch: num_train_examples]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "## Create a function that will use the prior functions to develop the predictions using probabilities and TensorFlow's prediction\n",
    "# Inputs\n",
    "    # X: The actual data\n",
    "    # Parameters: The dictionary of weights and biases \n",
    "\n",
    "def tf_prediction(X, parameters):\n",
    "    weights_1 = tf.convert_to_tensor(parameters[\"weights_1\"])\n",
    "    bias_1 = tf.convert_to_tensor(parameters[\"bias_1\"])\n",
    "    weights_2 = tf.convert_to_tensor(parameters[\"weights_2\"])\n",
    "    bias_2 = tf.convert_to_tensor(parameters[\"bias_2\"])\n",
    "    weights_3 = tf.convert_to_tensor(parameters[\"weights_3\"])\n",
    "    bias_3 = tf.convert_to_tensor(parameters[\"bias_3\"])\n",
    "    weights_4 = tf.convert_to_tensor(parameters[\"weights_4\"])\n",
    "    bias_4 = tf.convert_to_tensor(parameters[\"bias_4\"])\n",
    "\n",
    "    parameters = {\"weights_1\": weights_1,\n",
    "                  \"bias_1\": bias_1,\n",
    "                  \"weights_2\": weights_2,\n",
    "                  \"bias_2\": bias_2,\n",
    "                  \"weights_3\": weights_3,\n",
    "                  \"bias_3\": bias_3,\n",
    "                  \"weights_4\": weights_4,\n",
    "                  \"bias_4\": bias_4}\n",
    "\n",
    "    placeholder_x = tf.placeholder(\"float\", [X_train.shape[0], None])\n",
    "    keep_probability1 = tf.placeholder(tf.float32, name = \"keep_probability1\")\n",
    "    keep_probability2 = tf.placeholder(tf.float32, name = \"keep_probability2\")\n",
    "\n",
    "    z_linear_transform_4 = forward_propagation(placeholder_x, parameters, keep_probability1, keep_probability2)\n",
    "    p = tf.nn.softmax(z_linear_transform_4, axis = 0)  # dim = 0 because the classes are on that axis\n",
    "\n",
    "    tensorflow_session = tf.Session()\n",
    "    prediction = tensorflow_session.run(p, feed_dict = {placeholder_x: X, keep_probability1: 1.0, keep_probability2: 1.0})\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop the model! (Finally.)\n",
    "## Reading\n",
    "    # Improving NN with regularization: https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3\n",
    "    # Using the Adam algorithm: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "        # Using it as the optimizer/backpropogation: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py\n",
    "\n",
    "## Create a function that will actually train the model (3 layer NN)\n",
    "# Inputs and outputs for training the model\n",
    "    # X_train: The training set \n",
    "    # Y_train: The testing set \n",
    "    # X_test: The training set's validation \n",
    "    # Y_test: The testing set's validation \n",
    "    # learning_rate: The optimatization's learning rate (float)\n",
    "    # num_epochs: The number of Epochs for optimization (integer)\n",
    "    # minibatch_size: The size of the mini-batches (integer)\n",
    "    # print_cost: Print the cost every 10 Epochs \n",
    "    # Returns the parameters learned by the model which can then be used for prediction \n",
    "\n",
    "def train_model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1000, minibatch_size = 27, print_cost = True):\n",
    "    # Logistics \n",
    "    # Set seed to keep it same throughout...\n",
    "    tf.set_random_seed(1)  \n",
    "    seed = 3 \n",
    "\n",
    "    # Clear the default graph stack and resets the global default graph\n",
    "    ops.reset_default_graph()  # This is to be able to rerun the model and NOT overwrite the tensorflow variables \n",
    "    \n",
    "    # Getting things ready for the model...\n",
    "    (input_x, num_train_examples) = X_train.shape  # input size, number of examples in the train set\n",
    "    input_y = Y_train.shape[0]  # input_y : output size\n",
    "    costs = []  # Initialize costs list to append to \n",
    "    str(now)\n",
    "    time_0 = now.second # to mark the start of the training\n",
    "\n",
    "    # Placeholders with accurate shape (input_x, input_y)\n",
    "            # Probability of keeping a unit during dropout\n",
    "    X, Y = create_tf_placeholders(input_x, input_y)\n",
    "    keep_probability1 = tf.placeholder(tf.float32, name = \"keep_probability1\") \n",
    "    keep_probability2 = tf.placeholder(tf.float32, name = \"keep_probability2\")\n",
    "\n",
    "    # Initialize parameters using the function created...\n",
    "    parameters = initialize_tf_parameters()\n",
    "\n",
    "    # Forward propogation using the function created...\n",
    "    Z_linear_transform_4 = forward_propagation(X, parameters, keep_probability1, keep_probability2)\n",
    "\n",
    "    # Calculate the cost using the function created...\n",
    "    cost = calculate_cost(Z_linear_transform_4, Y)\n",
    "\n",
    "        # Regularization (refer to reading above why this improves outcome) \n",
    "    regularizers = tf.nn.l2_loss(parameters[\"weights_1\"]) + tf.nn.l2_loss(parameters[\"weights_2\"]) + tf.nn.l2_loss(parameters[\"weights_3\"])\\\n",
    "    + tf.nn.l2_loss(parameters[\"weights_4\"]) \n",
    "    \n",
    "    beta = 0.01  # Regularization constant\n",
    "    cost = tf.reduce_mean(cost + beta * regularizers)  # cost with regularization\n",
    "\n",
    "    # AdamOptimizer for Backpropogation\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training the NN loop...\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.  # Defines a cost related to an epoch\n",
    "            num_minibatches = int(num_train_examples / minibatch_size)  # number of minibatches of size minibatch_size in the train set\n",
    "            seed =+ 1\n",
    "            minibatches = generate_mini_batches_random(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\"\n",
    "                _, minibatch_cost = sess.run([adam_optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, keep_probability1: 0.7, keep_probability2: 0.5})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print(\"Cost after epoch {}: {:f}\".format(epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # Save the parameters out to a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"\\nTrained the Parameters!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z_linear_transform_4), tf.argmax(Y))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        train_cost = cost.eval({X: X_train, Y: Y_train, keep_probability1: 1.0, keep_probability2: 1.0})\n",
    "        test_cost = cost.eval({X: X_test, Y: Y_test, keep_probability1: 1.0, keep_probability2: 1.0})\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, keep_probability1: 1.0, keep_probability2: 1.0})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test, keep_probability1: 1.0, keep_probability2: 1.0})\n",
    "        \n",
    "        # Print how long this took\n",
    "        time_end = now.second\n",
    "        print(\"\\nTotal Training Time: %s sec.\" % (time_end - time_0))\n",
    "        \n",
    "        # Print the cost\n",
    "        print(\"\\nTraining Cost: \", train_cost)\n",
    "        print(\"Testing Cost: \", test_cost)\n",
    "        \n",
    "        # Print the accuracy\n",
    "        print(\"Training Accuracy: \", train_accuracy)\n",
    "        print(\"Testing Accuracy: \", test_accuracy)\n",
    "\n",
    "        # Plot out the loss/cost every iteration \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.title(\"Learning rate = {}, beta = {}, test cost = {:.6f},\\n\"\n",
    "                  \"train accuracy = {:.6f}, test accuracy = {:.6f}\".format(learning_rate, beta, test_cost, train_accuracy, test_accuracy))\n",
    "        \n",
    "        # Save the Plot\n",
    "        filename = \"_3LNN_{}_beta_{}_cost_{:.2f}-{:.2f}_acc_{:.2f}-{:.2f}\".format(\n",
    "            learning_rate, beta, train_cost, test_cost, train_accuracy, test_accuracy)\n",
    "        plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "        plt.savefig(directory_name + time_format + filename + \".png\", dpi=300)\n",
    "\n",
    "        return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190515_214956\n"
     ]
    }
   ],
   "source": [
    "# Logistics \n",
    "now = datetime.now()\n",
    "time_format = now.strftime(\"%Y%m%d_%H%M%S\") # Formatting date and time \n",
    "print(time_format)\n",
    "directory_name = \"/Users/makariousmb/Desktop/FAES_ML_FinalProject/NN_Output/\"  # Output Directory\n",
    "#filename = \"\" # Intialize filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 6.481701\n",
      "Cost after epoch 100: 0.925559\n",
      "Cost after epoch 200: 0.662635\n",
      "Cost after epoch 300: 0.603012\n",
      "Cost after epoch 400: 0.565957\n",
      "Cost after epoch 500: 0.545178\n",
      "Cost after epoch 600: 0.526692\n",
      "Cost after epoch 700: 0.516628\n",
      "Cost after epoch 800: 0.515195\n",
      "Cost after epoch 900: 0.511369\n",
      "\n",
      "Trained the Parameters!\n",
      "\n",
      "Total Training Time: 0 sec.\n",
      "\n",
      "Training Cost:  0.4584536\n",
      "Testing Cost:  1.7718256\n",
      "Training Accuracy:  0.9947289\n",
      "Testing Accuracy:  0.6360902\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAE0CAYAAAAv7IjLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XecZFWd///Xp0LnMN0zPcMwgSFHSQ6KIoiKAUUxJ1bBxBpYdXVXUTe43xV/rmnNgVVEEcU1IKyiYiIYEAYYEBwyAzNMjt09nas+vz/OqZmamqrq7pmurp6q9/PxqEd33XRO3br1ueeee+/nmrsjIiK1L1HtCoiIyPRQwBcRqRMK+CIidUIBX0SkTijgi4jUCQV8EZE6oYBfh8zsF2Z2frXrIfvGzFaa2VnVrofsPxTwp9FM+YG6+9nu/u1q1wPAzG4ws7dWodxuM7vazHaY2WNm9voy05qZ/ZeZbY6vT5qZ5Y0/0cxuN7OB+PfEScx7qZndb2ZZM7ugYh94z89UlW1xitd7Vdbd/kwBv8aYWaradciZSXUp4svACDAPOA/4qpkdW2LaC4GXAicAxwPnAH8PYGYNwDXAd4Eu4NvANXF42Xmju4B3AndM1Qeb4aZkvUf1tu72nbvrNU0vYCVwVolx5wDLgW3An4Dj88ZdDDwM9AF/A16WN+4C4I/AfwNbgI/FYX8APg1sBR4Fzs6b5wbgrXnzl5v2YOCmWPZvCD/Y75b4DGcCq4EPAuuAKwhB8GfAxrj8nwEL4/SXABlgCOgHvhSHHwX8On6e+4FXT/H30EoIOkfkDbsC+ESJ6f8EXJj3/i3ALfH/5wFPAJY3/nHgBePNW1DGH4AL9mJ7+lDcJrYC3wKaxtum4mfNAoNxvX8gDv9h/N62x+/82Jm63vd13dXrSy38GcDMTgYuI7ReZgNfB641s8Y4ycPA6UAn8B/Ad81sft4ingo8AswlBNHcsPuBOcAngW/mHw4XKDft94BbY70+CrxhnI9zANANHERooSUIgeggYDEhyHwJwN0/AtwMXOTube5+kZm1EoL99+LneR3wlVKtQDP7ipltK/G6u0QdjwAy7v5A3rC7gFItzWPj+GLTHgvc7THyRHcXjC8171Q4D3g+cCjhc/0LlN+m3P0NhJ3Si+N6/2Rc1i+Awwnr/Q7gylKFzoD1LntBAX9meBvwdXf/i7tnPPSvDwOnArj7D919jbtn3f0HwIPAU/LmX+PuX3T3MXcfjMMec/f/cfcMoZthPuEwupii05rZYuAU4N/cfcTd/wBcO85nyQL/7u7D7j7o7pvd/cfuPuDufYQd0jPLzH8OsNLdvxU/zx3Aj4FXFpvY3d/p7rNKvI4vUUYboRWbbzvQPsHptwNtcac43rLKzTsVvuTuq9x9C2Hdvi4OL7tNFePul7l7n7sPE3buJ5hZZ4lpq73eZS8o4M8MBwHvz28lAYuAAwHM7I1mtjxv3HGE1njOqiLLXJf7x90H4r9tJcovNe2BwJa8YaXKyrfR3Ydyb8ysxcy+Hk/Q9RK6CmaZWbLE/AcBTy1YF+cRjhymSj/QUTCsg9BtNZHpO4D+2Kofb1nl5p0K+d/HY8RthnG2qUJmljSzT5jZw/F7WhlHzSk2/V6ayvUue0EBf2ZYBVxS0Epqcffvm9lBwP8AFwGz3X0WcA+Q38qp1A9gLdBtZi15wxaNM09hXd4PHAk81d07gDPicCsx/SrgxoJ10ebu7yhWmJl9zcz6S7zuLVHHB4CUmR2eN+wEoNT098bxxaa9Fzi+oNV5fMH4UvNOhfzvYzGwJv5fcpuK4wvX++uBc4GzCF2HS+Lwoq3pGbDeZS8o4E+/tJk15b1ShID+djN7arwUrdXMXmRm7YQTXU446YmZvYnQwq84d38MWAZ81MwazOxpwIsnuZh2Qr/9NjPrBv69YPx64JC89z8DjjCzN5hZOr5OMbOjS9Tx7XGHUOxVtL/X3XcAPwH+X1zXpxGC3RUlPsN3gPeZ2QIzO5CwE7s8jruBcOL53WbWaGYXxeG/m8C8xPXaRAisuW0jEcedaWbj7czfZWYL47r9MPCDOLzcNgV7rvd2QpfPZqAF+Hi5QmfAep+KdVd/KnEmWK/iL8Jhshe8PhbHvQC4jXBFxVrCFRPtcdwlhCtWNgGfBW6k4CqbgnKKDXPgsPj/DePMnz/toYQTq33Ab4FLgW+W+HxnAqsLhh0Yy+sntPD+Pi4/Fcc/LQ7fCnwhDjsS+DlhJ7eZEDxPnOLvohv4KbCDcALz9XnjTid0HeTeG+Fk9pb4+iS7X5VzEnA7Ycd2B3DSJOa9ocg2cWYc9wbgT+NsT7mrdLYRzr+05I0vt02dGz/3NuCfCF1418Tv+THgjfnbwQxd73u97ur1ZXHliEyImf0AuM/dC1vqMsXM7BvAD939V9Wuy/5G6644BXwpy8xOIbSuHiVcc/5T4GnufmdVKyYikzaT74SUmeEAQr/rbMJNVe9QsBfZP6mFLyJSJ3SVjohInVDAFxGpEwr4MiE2Rel0zewCM/vDVNRpEmVebmYfq9CyJ5yi18zuLbhBaczM/i+OO73IDUxuZq+I448zs1+Z2aZi15eb2RIzu87MtprZOjP7UrzHAzM7wsyuMbONZrYlLufIgvkPMbOfmVlfLOOThWXI/k8BX2TfTDhFr7sf6/HmJMKNTo8Tro3H3W/2vJuXCDmF+oFfxtlHgf8lZIws5ivABkIepBMJ+YreGcfNIuRAOpKQT+lWwjX3wM4Uz78m3O9wALCQkO5Zak21bwTQa+a/KJ1O91RCCttthMB3Zt48FxAyePYRLuk8DziakAo5E5ezrUR53YQMm2sIN2T9NG/c24CHCJeKXgscGIcbIUX0BkKSrbsJdyRfSAiWI7HM/6vQOppUil5CQO4HWkuM/xbwrSLDDws/2z2GrwBemPf+U4TkaaXWrxNSdRDX0c3V3s70qvyr6hXQa/94UZDLH1hAuAv2hYQjxefG9z2EdBC9wJFx2vnE3OoUubO3SFk/J6QI6ALSwDPj8GcT7jY+GWgEvgjcFMc9n3C366wY/I8G5sdxlxPvaC5T5t1xx1Xs9ZUJrJ/JBvzLgMtLjGuJO8ozi4wrFfDfTkhF0BK/m3vIe25CwbQvBdYW1OUKQnrkTYQ7WJ9U7W1Or6l/6Tp82Vt/B1zn7tfF9782s2WEHcCPCEcEx5nZ4+6+lnBr/7gs5Pk/m9D63BoH3xj/ngdc5iFlMmb2IWCrmS0htOLbCQ9PudXdV0zmw3jplL5TzkIyulcCLykxySsIgffGEuOLuZFw9NMLJAlpFn5apOyFhIfYvC9v8ELgWbE+vwXeQ3hq11HuPjKJOsgMpz582VsHAa+y3dPvPoPQqt4BvIbQ6lxrZj83s6MmuNxFhJTMW4uMO5CQ5wUAd+8nHFUscPffER6s8mVgfTyZWpiKd6Z4OaFLqlRAPx/4jrtP6CaZmDDsV4Qb5FoJKY27gP8qmK4HuJ5wxPL9vFGDhKOuX8QA/2nCjXZFE9bJ/ksBXyaqWBrjK3z39Lut7v4JAHf/lbs/l9Cdcx8he2Ox5RRaRUjJPKvIuDWEHQ0AFp6ONZvwiEHc/Qvu/mTCU5GOAP55gmUWu4Im//W18eafpJIB3cwWEZLQfWcSy+sm7Ci/5OHBM5sJ5wBemLfcLkKwv9bdLymY/24ql2JbZhAFfJmownS63wVebGbPt/DwjKaYknahmc0zs5fEgDxMODmZyVvOQtv1kO/dxO6fXxAea9gV0yPncuh/D3iTmZ1o4fGPHwf+4u4rLaRQfqqZpQmZGHMnh4vVvVi5O6+gKfJ6e6n5yqXoLTF9rvvk2yUmyWV5fLhgPovlNMT3TXEd4O6bCCfG32FmqbizPJ/4eMB4pPMr4I/ufnGRMr8LnGpmZ1l4MM17CV1KK+L8l5vZ5aU+k+xHqn0SQa/940VBOt047KmEbokthFTGPyc8hGN+HL49Tn8DcEycpyFOtwXYVKKsbkJAXE+4SucneePeTnjG7xZ2fyD6cwgt1X5CsLoSaIvjDmfXw7x/OsXr5QZKp+g9D7i3YPoPUeaKGMLR0FuKDF9SpJyVeeNPjHXZGj//D4G5cdz5cfodcf3kXovz5n854eqn3ricY/PG/RZ4W7W3Qb32/aVcOiJSUjwSuws43t1Hq10f2TcK+CIidUJ9+CIidUIBX0SkTijgi4jUiboJ+Gb2NTP712rXQ0SkWvaLgD8VqXnd/e3u/p9TVSfZxcwazewyM+uNqXnfN860/21ma2Iq36/Ea+dz4482s9+Z2XYze8jMXlZiOf8e0weflTesXPrhsimCx6vXOJ//TDNbPZFpJ7CsG8zsrVOxrHoV79O43cwG4t8Tx5n+tWa2wsx2mNnDZnZ6HH6MmS2L28NWM/uNmR1TMO/JZnZT3N7Wm9l78sYtMbPfx3rcV7Ctjvc76Dazq2OdHjOz10/FutkvAv54LOb9rlXxZpiZ7KOEa90PItxU9AEze0GJaS8GlhIyWR5BSIT2L7Dze7yGcH19NyGL43fN7Ij8BZjZoYRcNLvl5/Ey6YcZJ0VwuXrJLjP9txYvI72GcDNZF+F+jmtK3ehnZs8lpKB4E2GbOYOQ5RXCnd2vJGyLcwjbz1V5884hpK/+OuGO78MIdzPnfB+4M477CPAjC+ktYPzt7cuEDK/zCPdzfNXMjp3Uyiim2jcCjPeiSGpedt2E8hbCjzqXMfGHwDrCDT83sfvNI5cTMyYSbl1fDbyfkE53LfCmMnV4E+Guwz7CxvD3BePPJdzY00u4KegFcXjRNL8UyRgZP89heXX9KnAd4WaZs4AXETaeXkL6gY8WzP8MdqUqXhXLOIVw81Iqb7pXAMun+Dt6Anhe3vv/BK4qMe0y4FV5718PrIr/Hxe/Y8sbfz3wnwXL+AUhbcBK8jJ4FkwzXvrhwhTBJes1zmdvjdtmll03NB1IaExdHLeHzYRc9t1xniZCQNocv6/bCD/sSwh3Bw/F5XypRJnltvNm4DOEnEPbCVk8m0ttI3H4DcBb85ax2/YZ19O7gAeBR+Owz8dl9BKylJ6eN30S+HD87H1x/CJCEPtMwWf5P+C9U7gtPi9uj/nb0OPE32SR6f9EkRvdikyXiutgIG/YxwnpRYpNfwThLvP2vGE3A2+fwO+glRDsj8gbfwXwiX1dPzO+he/ubyB8YS/20HrLfxLPMwkJnp4f3/+C0NKcS3ggxZVlFn0A0ElIJfsW4MsW8o0Us4HwQIoOQvD/bzM7GcDMnkLIe/LPhFbkGYRABOFLaiHkdplLyNc+Ua8nBIB2wo92B/DGWMaLCLfRvzTWYXH87F8kpCc+kRDUbyMElefmLffvYr32YGYXW14ytMJXiXm6CAHurrzBd8XPXHSW+Mp/v9DMOguG548/Lq+8VwEjvitLZynnAz/ykMitmDOAdR7yzoxXr5Li8s8G1viuVAxrgHcT0hA/k7B+thICXq5unYQgOJtw9/Cgu3+EEBQuisu5qESx5bbzTwNPBp5O2Kl9AMiW2kbKfbYCLyXcWZ3r0rgtLqObkPLihzH1A4RMnK8j7JQ7gDcDA4TW9usspp6ILeTnEFrCezCzu8tsj18pUc9jgbs9Rsnobopsj/HIeSnQE7sPV1t4UlhzwXTbCDvhLxKCfM6pwBYz+5OZbTCz/4vrOVePR9y9L2/6/N9Fue3tCCDj7g+UmHfvTdWetZIv9szFvoTQ6jikzDyz4jSd8f3l7N7CH2T3lu8G4NQJ1uenwHvi/18H/rvINPMJrb6uIuMuYPwW/nfGqcPncuUSbte/usR0HwSujP93E35486fwu1kU696UN+y55N32XzD9x4A/EoLOAcBf4vzzCbnvHyEEqTShtTYC/CrO20ZoZR5cbLvIK6OF0PI8s0QdFhJaga+bSL0msA7OBFYXDFsBPKdgexgltBTfTGhZHl9kWTeQ19qeQNk7t3PCUcUgcEKR6cptI7uVWbh9xuU/e5x6bM2VC9wPnFtiuhXAc+P/FxFSbE9lrPhXCo4uCTvEjxaZ9sD42ZbF72dO3AYuKTJtK+EJYi/KG/YA4WjpFMJR2xcI+Yog5ES6pWAZlxCfgTDO7+B0QmMkf963ATfs6/qZ8S38cazK/WMhgdcn4kmXXna1sueUmHezu4/lvR8gBJQ9mNnZZnZLPNm3jdByyS13EeHQtVC5NL8TsSr/jYXEYL+PJx23E1qF49UBdiU5awNeTcjjMqHc9BPUH//mpyLuIBzKF3MJoWtqOSHo/ZQQCDd4uHX/pYQjmHWELrf/JXS/AfwH4RD60XHqVDL9sJVOEVyyXuOUVcpBwNV5R0crCN018whHWL8Croon7T45iRPE5bbzOYTAU2p7LLWNTETh9vj+eKJze/x8nUxse/w24SgTyhxt7oN+dt8WofT2OBj/ftHd13pIQvdZ8rKM5ng4kvsa8B0zm5s3/9Xufpu7DxG2z6fHVvp49Si3vU3mM0zK/hLwS+V/yB/+ekJf+lmEjW9JHF6sm2DCLGQk/DHhUHmeu88i9K3nlrsKOLTIrOXS/O4gtEJzZRxQZJrCz/w9wkmjRe7eSdj4xqsD7v4E8GfgZYRWR8kfmJl92EqnCO4vNk/coa0FTsgbfAJwb4npB939Indf4O6HELqcbnf3TBx/t7s/091nu/vzCVkub42zPwd4t4UrgdYRAsv/mtkHC4o5nyLph61MiuDx6jWOYtvnKuBs3z19dJO7P+Huo+7+H+5+DKHr5RxCd12pZeUrt51vInQ9lNoei24jFGyPhBZnoZ31ilexfJDQgOiKv4ntTGB7JDRAzjWzEwjdsXs8pCWvnL1JWX0vcLyZ5f/uj6fI9hi33dWMv85zEux6ohjsmVY697/F8g4xs/a88Tt/F+Nsbw8AKTM7vNi8+2QqD6cq9QJuAS7Me78krtz8Lpl3EvaWHYTDr6+wZzfJbidtC8pYSfHugXZCy+yZ8Ys8m3A0kFvWUwiHdc8hbBALgKPiuJ8TAnXuUX1nxOG5EzonElpkXytV17x6bADOzytzA/Dd+H4xYe//akKXwWzgxLx5zwP+SujmKHoScx+/n08QWtNdhCdOraX0SbIFhENpI/SBrmL3E77Hx3XSAvwTIe1vYxw3mxCMcq9VwKuIWTHjNAuBMeDQgnI7CDuOUidCx6vX5ZR+JOFRhNZeZ96wfyR0lRwU3/cQuzkIVzI9iXBys5vQP3tBHHcV8PEy63q87fzLhOyWB8blP43wOMiS2wihtXlDXOeHEbrNCrt0Dst7/0LChQgHELKf/hvhN3JWHP/PhGB4eFyfxxNPjsfxv47jL6vAtthAOGH9nvi5L4rvG0pM//8I5yPmErbfm4kXCRC6Jk+K67GD0GWzhth9SXjk5lbC7zhNOEd3c96ybyE0FJsIDa5tQM8Et7erCOc2WoHTCDvUY/d5/Uz1Cq/Ei4LUvBQP+G2Ey7H64hf8RqYg4Mdx7yJc7bKN0EK+iryAHL/Mu2PZDwHPj8PLpfn9CKFFtopwaDtewH9l/Fx9hMsWv0QM+HH86YR+wNxVPOfnjcv1aX+7Qt9PI+G5qL3xs74vb9xi8lLxsuuk9gChr/e8gmV9Kq6rfsJJxsPKlLvHd0aJ9MOMkyJ4AvUqmyI4fv7cVTe5q3TeF5fVR+ji+Hic9nVx+I64vr5A3JYJAfqBuA6+UKSc8bbzZsL5nSfYdRVP7iqdotsIoSvm+rjMPxIusy0X8JPAN+Ny1hLOuez8LuL4fyHsrPsIAXVh3vy57f1ZFdoeTyJcGTRIOKl9Ut64DwO/yHufJuw0txG6Eb/AroD+KkK66n5C+u/rKDjvArwjruuthCuOFuWNW0LYkQ7G7zv/POR421s34ehnByH2vX4q1o2yZdYJM3uYcDnpb6pdl/2NKUXwlLLwQJvvAkvcPVvt+tSTGX0ThUwNM3sFoUX1u2rXZX/k4Tmver7rFIgnp98DfEPBfvop4Nc4M7uBcO30G/QDk2oys6MJl0DeRbifRaaZunREROrE/nJZpoiI7KMZ1aUzZ84cX7JkSbWrISKy37j99ts3uXvP+FPOsIC/ZMkSli1bVu1qiIjsN8zssYlOqy4dEZE6oYAvIlInFPBFROqEAr6ISJ1QwBcRqRMK+CIidUIBX0SkTtREwP/Cbx/kxgc2VrsaIiIzWk0E/K/d+DA3K+CLiJRVEwG/KZ1keEyJIEVEyqmJgN+YSjA0OpFHj4qI1K+aCfhq4YuIlFcTAT906aiFLyJSTk0EfLXwRUTGVyMBP6k+fBGRcdRGwE+rhS8iMp7aCPipJMOjCvgiIuXURsBPJxjSSVsRkbJqI+CnEmrhi4iMoyYCvu60FREZX00E/HBZprp0RETKqWjAN7NZZvYjM7vPzFaY2dMqUY5O2oqIjC9V4eV/Hvilu7/SzBqAlkoU0phKMJLJks06iYRVoggRkf1exQK+mXUAZwAXALj7CDBSibKa0kkARjJZmhLJShQhIrLfq2SXziHARuBbZnanmX3DzFoLJzKzC81smZkt27hx73LaN6bCx1C3johIaZUM+CngZOCr7n4SsAO4uHAid7/U3Ze6+9Kenp69KqgxHT6GrsUXESmtkgF/NbDa3f8S3/+IsAOYck2p0I2jFr6ISGkVC/juvg5YZWZHxkHPAf5WibJyLXxdmikiUlqlr9L5B+DKeIXOI8CbKlFIY66Fr5uvRERKqmjAd/flwNJKlgG7TtoqRbKISGk1cadt7rJMtfBFREqriYC/87JM9eGLiJRUGwE/d1mmrtIRESmpJgL+zssy1cIXESmpJgL+zssy1cIXESmpNgK+LssUERlXjQR8XZYpIjKemgr4auGLiJRWEwE/lUyQSphO2oqIlFETAR/0IHMRkfHUTMBvSieVHllEpIyaCfhq4YuIlFc7AT+d1ElbEZEyaifgpxI6aSsiUkbtBPx0Url0RETKqJ2Arxa+iEhZNRbw1cIXESmlZgJ+k7p0RETKqpmAry4dEZHyaijgJ3UdvohIGbUT8NPqwxcRKadmAn5TKsmw0iOLiJRUMwFfLXwRkfJqJ+CnEoxksmSzXu2qiIjMSKlKLtzMVgJ9QAYYc/ellSqrKR0ecziSydKUSFaqGBGR/VZFA370LHffVOlCck+9GhzJ7Az+IiKyS8106bQ3pQHoGxqrck1ERGamSgd8B643s9vN7MJiE5jZhWa2zMyWbdy4ca8L6mgKByu9Q6N7vQwRkVpW6YB/mrufDJwNvMvMziicwN0vdfel7r60p6dnrwvqaA4tfAV8EZHiKhrw3X1N/LsBuBp4SqXK6ohdOr2D6tIRESmmYgHfzFrNrD33P/A84J5KldeuLh0RkbIqeZXOPOBqM8uV8z13/2WlCtvZpTOogC8iUkzFAr67PwKcUKnlF2pvTGGmq3REREqpmcsyEwmjrTGlLh0RkRJqJuBDOHGrk7YiIsXVVMBvb1ILX0SklJoK+B3NaZ20FREpobYCflOaXp20FREpqrYCfnOKPnXpiIgUVVsBv0ldOiIipdRYwE/RNzymh6CIiBRRWwG/OY079I+oH19EpFBtBfwmpVcQESmltgJ+c8gUofQKIiJ7qqmA364WvohISTUV8Hd26aiFLyKyh9oK+LFLRy18EZE91VbAb9JjDkVESqmpgJ976pVO2oqI7KmmAn4qmaClIakuHRGRImoq4AN0NqfZpoAvIrKHmgv43a0NbNkxUu1qiIjMODUX8Ge3NbJZAV9EZA+1F/BbG9jcP1ztaoiIzDg1F/DVpSMiUlzNBfzZbQ0MjGQYHMlUuyoiIjNK7QX81gYANu9Qt46ISL6KB3wzS5rZnWb2s0qXBTC7tRGAzf3q1hERyTcdLfz3ACumoRwAuttCC1/9+CIiu6towDezhcCLgG9Uspx8c2ILf5Ou1BER2U2lW/ifAz4AZEtNYGYXmtkyM1u2cePGfS5QLXwRkeIqFvDN7Bxgg7vfXm46d7/U3Ze6+9Kenp59Lre1IUlDKqGbr0REClSyhX8a8BIzWwlcBTzbzL5bwfIAMDPmtDbopK2ISIGKBXx3/5C7L3T3JcBrgd+5+99Vqrx83W0NuixTRKRAzV2HD+HSTPXhi4jsbloCvrvf4O7nTEdZkMuno4AvIpKvNlv46tIREdlDTQb87tZGhkazDIzoUYciIjk1GfBnx2vxN/WpW0dEJGdCAd/MrpjIsJmipz3cbbuxf6jKNRERmTkm2sI/Nv+NmSWBJ099dabG3BjwN/SqH19EJKdswDezD5lZH3C8mfXGVx+wAbhmWmq4F+a2NwGwoU8BX0Qkp2zAd/f/z93bgU+5e0d8tbv7bHf/0DTVcdJmtzaQTBgb+tSlIyKSM9EunZ+ZWSuAmf2dmX3WzA6qYL32SSJhzGlrUJeOiEieiQb8rwIDZnYCIfvlY8B3KlarKTC3vUldOiIieSYa8Mfc3YFzgc+7++eB9spVa9/NbW9UwBcRyTPRgN9nZh8C3gD8PF6lk65ctfbd3I5GNqoPX0Rkp4kG/NcAw8Cb3X0dsAD4VMVqNQV62hrZvGOEsUzJZ6+IiNSVCQX8GOSvBDrjg02G3H1G9+H3dDThDpuURE1EBJj4nbavBm4FXgW8GviLmb2ykhXbVztvvlK3jogIAKkJTvcR4BR33wBgZj3Ab4AfVapi+0p324qI7G6iffiJXLCPNk9i3qqY26G7bUVE8k20hf9LM/sV8P34/jXAdZWp0tToaVOXjohIvrIB38wOA+a5+z+b2cuBZwAG/JlwEnfGakgl6GpJq4UvIhKN1y3zOaAPwN1/4u7vc/d/JLTuP1fpyu2reR1NrNuuFr6ICIwf8Je4+92FA919GbCkIjWaQou6W1i9daDa1RARmRHGC/hNZcY1T2VFKmFxdwurtgwSskKIiNS38QL+bWb2tsKBZvYW4PbKVGnqLOpqZnA0o5uvREQY/yqd9wJXm9l57ArwS4EG4GWVrNhUWNTdAsCqrQM7H3soIlKvygZ8d18PPN3MngUcFwf/3N1/V/GaTYHFuYC/ZYCTF3dVuTYiItU1oevw3f33wO8ns2AzawJuAhpjOT9y93+fdA33wcKuXQFfRKTeTfTGq70xDDzb3fvNLA38wcx+4e63VLANOwMJAAAVuklEQVTM3TQ3JJnT1siqLYPTVaSIyIxVsYAfH5jSH9+m42vaL5dZ3N3MKl2aKSJS2Xw4ZpY0s+XABuDX7v6XItNcaGbLzGzZxo0bp7wOi7pbeFxdOiIilQ347p5x9xOBhcBTzOy4ItNc6u5L3X1pT0/PlNdhUVcLa7cP6UEoIlL3piXjpbtvA24AXjAd5eVb3N1CJuus2aYUCyJS3yoW8M2sx8xmxf+bgbOA+ypVXimLZ4crdR7bsmO6ixYRmVEq2cKfD/zezO4GbiP04f+sguUVdfCcVgBWblLAF5H6VsmrdO4GTqrU8idqbnsjLQ1JHlHAF5E6N6OfWjUVzIwls1t5VAFfROpczQd8gIN7WtWlIyJ1ry4C/iFzWlm1dZBRXZopInWsLgL+ktmtZLKunDoiUtfqIuAf3BOu1FE/vojUs/oI+LMV8EVE6iLgd7U2MKslrYAvInWtLgI+hBuwHtmogC8i9atuAv5hPW08tLF//AlFRGpU3QT8w+e1sbFvmG0DeqC5iNSn+gn4c9sBeGiDWvkiUp/qJuAfNrcNgAcV8EWkTtVNwF8wq5mWhiQPrO+rdlVERKqibgJ+ImEcNrdNXToiUrfqJuBD6NZ5cL0CvojUp7oK+IfPbWdd7xC9Q6PVroqIyLSrs4AfT9yqH19E6lBdBfwnLewEYPmq7VWuiYjI9KurgD+vo4kFs5q54/Gt1a6KiMi0q6uAD3DS4lnc+ZgCvojUn7oL+Ccv7mLN9iHWbR+qdlVERKZV/QX8g7oA1K0jInWn7gL+MfM7aEwluEPdOiJSZ+ou4DekEjxpQSe3q4UvInWmYgHfzBaZ2e/NbIWZ3Wtm76lUWZN18kFd3PtEL8NjmWpXRURk2lSyhT8GvN/djwZOBd5lZsdUsLwJO3nxLEYyWe55orfaVRERmTYVC/juvtbd74j/9wErgAWVKm8yTl4cTtzeqW4dEakj09KHb2ZLgJOAvxQZd6GZLTOzZRs3bpyO6jBXN2CJSB2qeMA3szbgx8B73X2PPhR3v9Tdl7r70p6enkpXZ6eTD+rijse2TVt5IiLVVtGAb2ZpQrC/0t1/UsmyJuvkxbNY1zvEmm2D1a6KiMi0qORVOgZ8E1jh7p+tVDl768nxBqzbVm6pck1ERKZHJVv4pwFvAJ5tZsvj64UVLG9Sjj2wk/amFH9+eHO1qyIiMi1SlVqwu/8BsEotf18lE8bTDpnNHx7aVO2qiIhMi7q70zbfaYfNYfXWQR7fPFDtqoiIVFzdB3yAPz6sVr6I1L66DviH9rQyr6NR3ToiUhfqOuCbGWcc3sNND2xUXh0RqXl1HfABzjnhQPqGxrjh/um5y1dEpFrqPuCfduhsZrc2cO3yNdWuiohIRdV9wE8lE5xz/Hx+s2I9fUOj1a6OiEjF1H3AB3jJiQsYHsty/b3rq10VEZGKUcAn5NVZ2NXMtXepW0dEapcCPuFqnXNPPJA/PLSJTf3D1a6OiEhFKOBHLzlhAZmsc91f11a7KiIiFaGAHx15QDtHHdDO1Xc+Ue2qiIhUhAJ+nlecvJA7H9/G39boWbciUnsU8PO8eukimtNJvv2nldWuiojIlFPAz9PZkuZlJy/gp8ufYOuOkWpXR0RkSingF7jg6UsYHsvyrT8+Wu2qiIhMKQX8AkfMa+dFx8/n0psfYd32oWpXR0RkyijgF3HxC44im4VPX39/tasiIjJlFPCLWNTdwgWnLeHHd6zm3jXbq10dEZEpoYBfwruedRizmtNc8vMVuHu1qyMiss8U8EvobE7znucczp8e3sxvV2yodnVERPaZAn4Z5516EIf0tHLJdSsYGctWuzoiIvtEAb+MdDLBv55zDI9u2qGbsURkv6eAP45nHTmXZx81l8/95gHuW6eUCyKy/6pYwDezy8xsg5ndU6kypsvHXnocbU0pLrjsNtZsG6x2dURE9kolW/iXAy+o4PKnzYGzmrn8TU9hx/AYF3zrVrYP6FGIIrL/qVjAd/ebgC2VWv50O3p+B19/45N5dNMO3nbFMoZGM9WukojIpFS9D9/MLjSzZWa2bOPGjdWuTllPP3QOn3n1idz66Bbe97/LyWR1fb6I7D+qHvDd/VJ3X+ruS3t6eqpdnXG95IQD+ZcXHc11f13Hu6+6U907IrLfSFW7Avujt55+CKMZ5zPX38/tK7fyqVcdz+mHz/ydlYjUt6q38PdX7zjzUH7yzqfT1pTiDd+8lX+75h4GRsaqXS0RkZIqeVnm94E/A0ea2Woze0ulyqqW4xfO4mf/8Aze8oyD+c6fH+NFX/gDNz84s89DiEj9spmUGGzp0qW+bNmyaldjr/zp4U186Cd/5bHNAzzvmHn8y4uOYfHslmpXS0RqnJnd7u5LJzStAv7UGR7L8I2bH+VLv3uI0UyWw+a28fRD5/Cm05awqFvBX0SmngJ+la3bPsQVt6zkr0/08ueHNzGacRZ2NXPqIbN533OP4MBZzdWuoojUCAX8GWTd9iF+fMdqVqzt5dd/Ww/AMw6bw+Hz2nGcJy3o5IwjeuhoSle5piKyP5pMwNdlmRV2QGcT73rWYQCs3jrA1298hD8+tIkbH9iIGYxmnFTCOGVJN8ct6GBhVwuLupuZ1dJAT1ujuoJEZMoo4E+jhV0t/OdLj9v5PpN17nx8K79ZsYEbH9jId/78GMMFefePX9jJCQtnAbBy8w562ht51pFzWdjVzMFzWpnV0jCtn0FE9l/q0plB3J1N/SOs3jrA9sFRHtrQzzXL17B66wBZh8XdLazaOsC2vLt7F3e3MDKWZWgsQ0MywdMOnc1TDu6mOZ2kIZWgrTHF/M5m5s9qor0xhZlV8ROKyFRTH34NG8tkuW9dH+t7h7hvXR9/W9tLSzpJc0OS3sFRbnhg4247hHztjSkWdrewZccwm/pHaG9KMa+9iYVdzSzoaiaVSJDJZlnY1UJ7U4qMO9msM6etkeMWdNLRlGY0m2XLjhE29Q/T0ZTmmPkdJBLaiYhUi/rwa1gqmeC4BZ0ct6CT5xw9b4/xY5ksG/qGGRnLMpLJsn1wlHXbh1i3fYjVWwd4fMsAR89v54COJnqHRlm3fZgntg1y28otZB0M6Bue+B3DPe2NzG1vJJ1M0JBMkEoa6WSCdNIYyzqpRIJjDuwglTA29Q+zuX+EpnSSBV3NLJjVxIJZLfS0NzKayTIwkmFkLEtnc5rmhgTusHh2C+lEgie2DdLamKKrJV3yKCWbde18RMpQwK8xqWRiny/73DYwwsBIhmTCMIM124a4b20vO0YypBJGd2sDs1sbWLt9iJsf3Ejf0BgjmSyjmSwjY1l2DI8xknHSSWNgJMNv71uPO3Q0pZjT1sjASIb1fUNM5OCyIZmgIZWgP+6EGpIJetobaUwlSCSMhEHCjKHRDKu3DnJAZxNHzmvniW2DNKWTLOpuIWnggHvur2NmtDWm6GxO09mcpqM5RcKM3sFRmhuSJMzYumOEeZ1NLJjVzGgmy5y2RuZ2NDI8GnZOY9ksXS0N7BgeY9vgKAu7mmlvSpPJhiOjsayTMOhubcDMcHd6h8YwQ1dlSVWoS0cqbjDuPBpSuzJ5jGayrNs+xBPbBtnYN0xjKkFLQ4p00tg+OMrQWBZ3529rehkazXDkAR0Mj2VY3zvMhr4hRsayuEPWnUzWSacSLJzVzGObB3hkUz+LuloYjDsBxzHCzsHMMMJ8/cNjbB8cZTRT2d/ArJY0HU1p1vcO7TwpP6+jkUzWGRjZ/bkKBsxpb2ROWyMjY1kaUwnam1J0NIdlNKQSbOgbpn8odNt1tTSQSBg7hsfoHx6LO9ssT1rQydz2Jh7fMkBHU4qsw/3r+ljY1cyhc9voGxqjqyXNrJY067YPs2bbIL1Doyye3UJTKknWnXkdTTSkEoxmQjdfOmGs3T7Eut4h3J35nc0cOKuZzTuGueWRzRzU3cohPa1sHRhl644RMu4c1tNGS2OSbJadXYSZrJNxxz0cAR41v51tA6MsX7UtHgEmWNDVTP/QGKlkgoNmt9DVkmY046zvHaK9KU1XS5qsO49vGWBoNMspS7ppa0wxOJohacZDG/tZvXWAhV0tdDaHabcNjJJ1p6UhSUtDkuaGFM3p8D9A31BYf8mE7fzsmayzfXCUbQMjDMejz87mNI2pBH1DY7Q1pUgnEwyPZRgcyZBIGG0Nqd2ONN13feZ0IjRUhkYzjGSyU7LjVx++yAS5O0Ojoesr605Hc5rBkQxZdzqb06zZNsj63mHSSWND3zCb+4dpSidpaUiRTMDWgVFaGpJ0NqdZvXWQgZExEmakEkYyYYxmnAc39LFjOMO8jkbmdTQxksny8IYdNKYTtKST5PdQZbKwsX+YLTuGaUgmGB7L0js0St/QGL2DowyNZulpb6SzOQSKLTtGyGSdtqYUrY0p2hrD0cnyVdvoHx7jwM5meodGyWadIw9o5/EtA2zqHyGdtN12dHPbG2lrTLF66yAjmSxmTOgILKc5nWSwxh4KlEwYWfey66EhmaC7tWG3I9ZkwkgnjWwWxrJZ8h+bkU4aPW2NrO8bJpN12ptSO5fx6/c9c6/qqT58kQkyM5obwknvnLbGXT+LQ3raOKSnrRpV2yeZrDOaydKUDp8r142V28E1pRP0xp3I3I5GGlPJnfPlbOgbYizjpJLGY5sHyGad+bOaOaCjCTNYu32INdsGaUonOHFRFxv7hlmzfZDulgZmtzWQdXhoQz8jY1mSCSOZCN1vyYTt/DswkuFva3tpa0xyypJu5nU0MTCSYc22QTqa04yMZXl8S7hqLWnGvI5G+obCkZkZLJjVTDJh3LZyC5ksNKcTjGWdhV0tHDynlSe2DdA3FHbCs1rSJC2UOTCaYXBkLPwfj7I64k5zLOOs3T7ESCZDMpGgqyVNV0sDDakEvYOj4Qh0NEt7U4oNfeGIc1HekcTWgRFGMx4+c/ycuVff0BgbeodY0NVMW2OKtduHGM1kaZ+mLj618EVE9mOTaeErH76ISJ1QwBcRqRMK+CIidUIBX0SkTijgi4jUCQV8EZE6oYAvIlInFPBFROrEjLrxysw2Ao/t5exzgE1TWJ2ponpN3kytm+o1OarX5O1N3Q5y956JTDijAv6+MLNlE73bbDqpXpM3U+umek2O6jV5la6bunREROqEAr6ISJ2opYB/abUrUILqNXkztW6q1+SoXpNX0brVTB++iIiUV0stfBERKUMBX0SkTuz3Ad/MXmBm95vZQ2Z2cRXrscjMfm9mK8zsXjN7Txz+UTN7wsyWx9cLq1S/lWb211iHZXFYt5n92swejH+7prlOR+atl+Vm1mtm763GOjOzy8xsg5ndkzes6Pqx4Atxm7vbzE6uQt0+ZWb3xfKvNrNZcfgSMxvMW3dfm+Z6lfzuzOxDcZ3db2bPn+Z6/SCvTivNbHkcPp3rq1SMmL7tzOPDhPfHF5AEHgYOARqAu4BjqlSX+cDJ8f924AHgGOCjwD/NgHW1EphTMOyTwMXx/4uB/6ryd7kOOKga6ww4AzgZuGe89QO8EPgF4ZnjpwJ/qULdngek4v//lVe3JfnTVaFeRb+7+Fu4C2gEDo6/2+R01atg/GeAf6vC+ioVI6ZtO9vfW/hPAR5y90fcfQS4Cji3GhVx97Xufkf8vw9YASyoRl0m4Vzg2/H/bwMvrWJdngM87O57e6f1PnH3m4AtBYNLrZ9zge94cAswy8zmT2fd3P16dx+Lb28BFlaq/MnUq4xzgavcfdjdHwUeIvx+p7VeZmbAq4HvV6LscsrEiGnbzvb3gL8AWJX3fjUzIMia2RLgJOAvcdBF8ZDssunuNsnjwPVmdruZXRiHzXP3tRA2RmBuleoG8Fp2/xHOhHVWav3MtO3uzYSWYM7BZnanmd1oZqdXoT7FvruZss5OB9a7+4N5w6Z9fRXEiGnbzvb3gG9FhlX1OlMzawN+DLzX3XuBrwKHAicCawmHk9VwmrufDJwNvMvMzqhSPfZgZg3AS4AfxkEzZZ2VMmO2OzP7CDAGXBkHrQUWu/tJwPuA75lZxzRWqdR3N1PW2evYvWEx7eurSIwoOWmRYfu0zvb3gL8aWJT3fiGwpkp1wczShC/ySnf/CYC7r3f3jLtngf+hQoex43H3NfHvBuDqWI/1uUPE+HdDNepG2And4e7rYx1nxDqj9PqZEdudmZ0PnAOc57HTN3aZbI7/307oKz9iuupU5rur+jozsxTwcuAHuWHTvb6KxQimcTvb3wP+bcDhZnZwbCW+Fri2GhWJfYPfBFa4+2fzhuf3ub0MuKdw3mmoW6uZtef+J5zwu4ewrs6Pk50PXDPddYt2a3XNhHUWlVo/1wJvjFdRnApszx2STxczewHwQeAl7j6QN7zHzJLx/0OAw4FHprFepb67a4HXmlmjmR0c63XrdNUrOgu4z91X5wZM5/oqFSOYzu1sOs5OV/JFOJP9AGHP/JEq1uMZhMOtu4Hl8fVC4Argr3H4tcD8KtTtEMIVEncB9+bWEzAb+C3wYPzbXYW6tQCbgc68YdO+zgg7nLXAKKFl9ZZS64dwqP3luM39FVhahbo9ROjfzW1rX4vTviJ+x3cBdwAvnuZ6lfzugI/EdXY/cPZ01isOvxx4e8G007m+SsWIadvOlFpBRKRO7O9dOiIiMkEK+CIidUIBX0SkTijgi4jUCQV8EZE6oYAvNcPM+uPfJWb2+ile9ocL3v9pKpcvMh0U8KUWLQEmFfBzN9+UsVvAd/enT7JOIlWngC+16BPA6TG/+T+aWdJC/vjbYlKvvwcwszNjfvLvEW5swcx+GhPM3ZtLMmdmnwCa4/KujMNyRxMWl32PhecNvCZv2TeY2Y8s5K2/Mt5piZl9wsz+Fuvy6WlfO1K3UtWugEgFXEzIyX4OQAzc2939FDNrBP5oZtfHaZ8CHOchZS/Am919i5k1A7eZ2Y/d/WIzu8jdTyxS1ssJicJOAObEeW6K404CjiXkP/kjcJqZ/Y2QcuAod3eLDy4RmQ5q4Us9eB4hJ8lyQjra2YScKQC35gV7gHeb2V2EHPOL8qYr5RnA9z0kDFsP3Aickrfs1R4SiS0ndDX1AkPAN8zs5cBAkWWKVIQCvtQDA/7B3U+Mr4PdPdfC37FzIrMzCQm2nubuJwB3Ak0TWHYpw3n/ZwhPqBojHFX8mPCgi19O6pOI7AMFfKlFfYRHyOX8CnhHTE2LmR0Rs4YW6gS2uvuAmR1FeKxczmhu/gI3Aa+J5wl6CI/XK5kFMuZC73T364D3ErqDRKaF+vClFt0NjMWumcuBzxO6U+6IJ043Uvxxjr8E3m5mdxMyOt6SN+5S4G4zu8Pdz8sbfjXwNEK2RQc+4O7r4g6jmHbgGjNrIhwd/OPefUSRyVO2TBGROqEuHRGROqGALyJSJxTwRUTqhAK+iEidUMAXEakTCvgiInVCAV9EpE78/5POLJqLfi3qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model and get learned parameters\n",
    "parameters = train_model(X_train, Y_train, X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1168834  0.11289364 0.00507009 0.00417556 0.1244908  0.61020917\n",
      " 0.00312868 0.02013838 0.00301026]\n",
      "(9, 986)\n"
     ]
    }
   ],
   "source": [
    "# Make Prediction on Test Data\n",
    "prediction = tf_prediction(X_test, parameters)\n",
    "\n",
    "# Logistics \n",
    "print(prediction[:,0])\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructuring for Kaggle Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.112894</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.124491</td>\n",
       "      <td>0.610209</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.020138</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.666512</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.260194</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.434978</td>\n",
       "      <td>0.083367</td>\n",
       "      <td>0.016015</td>\n",
       "      <td>0.094720</td>\n",
       "      <td>0.053327</td>\n",
       "      <td>0.131533</td>\n",
       "      <td>0.102707</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.036222</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.434978</td>\n",
       "      <td>0.083367</td>\n",
       "      <td>0.016015</td>\n",
       "      <td>0.094720</td>\n",
       "      <td>0.053327</td>\n",
       "      <td>0.131533</td>\n",
       "      <td>0.102707</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.036222</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.211680</td>\n",
       "      <td>0.072845</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>0.066144</td>\n",
       "      <td>0.122117</td>\n",
       "      <td>0.301289</td>\n",
       "      <td>0.038293</td>\n",
       "      <td>0.042915</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class1    class2    class3    class4    class5    class6    class7  \\\n",
       "0  0.116883  0.112894  0.005070  0.004176  0.124491  0.610209  0.003129   \n",
       "1  0.018677  0.666512  0.000660  0.000011  0.260194  0.002345  0.050450   \n",
       "2  0.434978  0.083367  0.016015  0.094720  0.053327  0.131533  0.102707   \n",
       "3  0.434978  0.083367  0.016015  0.094720  0.053327  0.131533  0.102707   \n",
       "4  0.211680  0.072845  0.033230  0.111487  0.066144  0.122117  0.301289   \n",
       "\n",
       "     class8    class9  id  \n",
       "0  0.020138  0.003010   1  \n",
       "1  0.000852  0.000299   2  \n",
       "2  0.047131  0.036222   3  \n",
       "3  0.047131  0.036222   4  \n",
       "4  0.038293  0.042915   5  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restructuring in Desired Output\n",
    "format_for_Kaggle_submission = pd.DataFrame(prediction.T)\n",
    "format_for_Kaggle_submission[\"id\"] = testing_index\n",
    "format_for_Kaggle_submission.columns = [\"class1\", \"class2\", \"class3\", \"class4\", \"class5\", \"class6\", \"class7\", \"class8\", \"class9\", \"id\"]\n",
    "format_for_Kaggle_submission.to_csv(directory_name + filename + \".csv\", index = False)\n",
    "\n",
    "format_for_Kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
