{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Medicine: Redefining Cancer Treatment\n",
    "\n",
    "- **By:** Mary B. Makarious\n",
    "- **Project:** FAES BIOF509 - Applied ML Final Project \n",
    "- **Date Written:** 04.05.2019 \n",
    "- **Last Updated:** 15.05.2019 \n",
    "\n",
    "**Description:** This notebook is intended for the ML final project for the NIH FAES BIOF509 - Applied ML course. The data used here is from a Kaggle dataset found here: https://www.kaggle.com/c/msk-redefining-cancer-treatment/data and this project is intended to explore different machine learning algorithms on cancer data. \"Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers)... Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually annotated into 9 different categories.\"\n",
    "\n",
    "**Data:** 2 different kinds of files:\n",
    "- Contains information about the genetic variants: `training_variants.csv` and `test_variants.csv`\n",
    "- Contains the clinical evidence (in text form) that was used to manually classify the variants `training_text.csv` and `test_text.csv`\n",
    "\n",
    "There is a class target feature corresponding to 1 of the 9 categories variants will be classified as in the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing + Setting Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import os #To set wd \n",
    "import re #To use regex\n",
    "import string \n",
    "import pandas as pd #Loading and manipulating data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# For NLP Preprocessing\n",
    "import nltk\n",
    "# nltk.download('stopwords') #Download stopwords if not already in environment\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import utils\n",
    "import tqdm\n",
    "import keras\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For NN\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# Data is too large to host on GitHub, so accessing it locally for now\n",
    "os.chdir(\"/Users/makariousmb/Desktop/FAES_ML_FinalProject/msk-redefining-cancer-treatment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading + Formatting + Merging the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in all the data, formatting, and generating Pandas dataframes\n",
    "\n",
    "# Read in the training variants file, which contains information on genetic variants \n",
    "train_variant_df = pd.read_csv(\"training_variants.csv\")\n",
    "#train_variant_df.head()\n",
    "\n",
    "# Read in the test variants file (also containing information on genetic variants)\n",
    "test_variant_df = pd.read_csv(\"stage2_test_variants.csv\")\n",
    "#test_variant_df.head()\n",
    "\n",
    "# Read in the training text (clinical evidence used to manually classify the variants)\n",
    "train_text_df = pd.read_csv(\"training_text.csv\", sep = \"\\|\\|\", engine = \"python\", header = None, skiprows = 1, names = [\"ID\",\"Text\"])\n",
    "#train_text.head()\n",
    "\n",
    "# Read in the test text (also clinical evidence used to manually classify the variants)\n",
    "test_text_df = pd.read_csv(\"stage2_test_text.csv\", sep = \"\\|\\|\", engine = \"python\", header = None, skiprows = 1, names = [\"ID\", \"Text\"])\n",
    "#test_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of training variants available is: 3321\n"
     ]
    }
   ],
   "source": [
    "# Merge the training data together\n",
    "\n",
    "# Merge\n",
    "train_data = pd.merge(train_variant_df, train_text_df, how = \"left\", on = \"ID\")\n",
    "#train_data.head()\n",
    "\n",
    "# Set up the data: Remove \"Class\" since this is what we will try to predict later \n",
    "train_y = train_data[\"Class\"].values #Return numpy representation of this dataframe for later \n",
    "train_x = train_data.drop(\"Class\", axis = 1) #axis=1 to drop actual column and not index value \n",
    "\n",
    "# Find the number of training variants \n",
    "training_size = len(train_x)\n",
    "print(\"The total number of training variants available is: %d\" % (training_size)) #3321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of available variants to test is: 986\n"
     ]
    }
   ],
   "source": [
    "# Merge the test data together\n",
    "\n",
    "# Merge \n",
    "test_x = pd.merge(test_variant_df, test_text_df, how  = \"left\", on = \"ID\")\n",
    "#train_data.head()\n",
    "\n",
    "# Find the number of test variants\n",
    "testing_size = len(test_x)\n",
    "print(\"The total number of available variants to test is: %d\" % (testing_size)) #986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID    Gene               Variant  \\\n",
       "0  0  FAM58A  Truncating Mutations   \n",
       "1  1     CBL                 W802*   \n",
       "2  2     CBL                 Q249E   \n",
       "3  3     CBL                 N454D   \n",
       "4  4     CBL                 L399V   \n",
       "\n",
       "                                                Text  \n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
       "1   Abstract Background  Non-small cell lung canc...  \n",
       "2   Abstract Background  Non-small cell lung canc...  \n",
       "3  Recent evidence has demonstrated that acquired...  \n",
       "4  Oncogenic mutations in the monomeric Casitas B...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all the data together \n",
    "\n",
    "# Index the values for later \n",
    "testing_index = test_x[\"ID\"].values\n",
    "\n",
    "# Merge\n",
    "all_the_data = np.concatenate((train_x, test_x))\n",
    "\n",
    "# Convert back to a pandas df\n",
    "all_the_data = pd.DataFrame(all_the_data)\n",
    "\n",
    "# Force column names\n",
    "all_the_data.columns = [\"ID\", \"Gene\", \"Variant\", \"Text\"]\n",
    "all_the_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with NLP\n",
    "\n",
    "The text needs to be converted to vectors so that ML can be applied. This is done using NLP.\n",
    "Instead of reinventing the wheel, I am using a modified version of the NLP script found here: https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to clean and generate sentences\n",
    "\n",
    "# Create a function that will clean the text\n",
    "    # This will remove any odd characters\n",
    "def textClean(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split() # Force the text to be lowercase \n",
    "    stops = set(stopwords.words(\"english\")) # Use the English stopwords (ex: I, me, my, etc.)\n",
    "    text = [word for word in text if not word in stops] # Join each word one by one as long as they are not the common English \"stopwords\"   \n",
    "    text = \" \".join(text) # Append\n",
    "    return(text)\n",
    "\n",
    "# Create a function that cleans the sentences \n",
    "    # Take in the text, processes it using the function above, and then generates a translation table with mapped characters\n",
    "    # This function takes in strings as inputs, so if need be force cast the column datatype to string for this to work\n",
    "def cleanup(text):\n",
    "    text = textClean(text) \n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) #.maketrans() : returns a translation table that maps each character in the intabstring into the character at the same position in the outtab string\n",
    "    return text\n",
    "\n",
    "# Create a function that will take in data and generate sentences that are labeled\n",
    "    # This will take in a dataset, generate a list of sentences, and split accordingly\n",
    "def constructLabeledSentences(data):\n",
    "    sentences=[]\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(TaggedDocument(utils.to_unicode(row).split(), [\"Text\" + \"_%s\" % str(index)])) # Index each sentence to do ML later\n",
    "    return sentences\n",
    "\n",
    "# TaggedDocument: TaggedDocument(namedtuple('TaggedDocument', 'words tags'))\n",
    "    # StackOverflow Ex: https://stackoverflow.com/questions/45125798/how-to-use-taggeddocument-in-gensim\n",
    "    # LabeledSentences is now deprecated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    cyclindependent kinases cdks regulate variety ...\n",
      "1    abstract background nonsmall cell lung cancer ...\n",
      "2    abstract background nonsmall cell lung cancer ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Clean up all the text\n",
    "\n",
    "# Convert the text column to a string type to be applied with the cleanup function \n",
    "all_the_data[\"Text\"] = all_the_data[\"Text\"].astype('str')\n",
    "\n",
    "# Clean up the text\n",
    "all_cleaned_text = all_the_data[\"Text\"].apply(cleanup)\n",
    "print(all_cleaned_text[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentences with the now cleaned text\n",
    "labeled_sentences = constructLabeledSentences(all_cleaned_text)\n",
    "\n",
    "# View the first sentence (index at the end)\n",
    "#print(labeled_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep and Feature Extraction\n",
    "Use Doc2Vec/Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Implementation Example: https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "# Doc2Vec Documentation: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "# \"Right Number\" of Epochs? : https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n",
    "\n",
    "## Time Spent on this Chunk\n",
    "# 5 Epochs = 10 min\n",
    "\n",
    "text_input_dimensions = 300 \n",
    "\n",
    "max_epochs = 5 # One Epoch is when the whole dataset is passed forward and backward through the neural network once\n",
    "alpha = 0.025 # alpha: the initial learning rate\n",
    "\n",
    "model = Doc2Vec(vector_size = text_input_dimensions, # Dimensionality of the feature vectors\n",
    "               alpha = alpha,\n",
    "               min_alpha = 0.00025, \n",
    "               min_count = 1, # Ignores all words with total frequency lower than 1\n",
    "               dm = 1,  #dm: =0: Bag of Words, =1: Distributed memory to preserve word order \n",
    "               window = 5, # The maximum distance between the current and predicted word within a sentence\n",
    "               sample = 1e-4, # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "               negative = 5,\n",
    "               workers = 4)\n",
    "\n",
    "model.build_vocab(labeled_sentences)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"Iteration {0}\".format(epoch))\n",
    "    model.train(labeled_sentences,\n",
    "               total_examples = model.corpus_count,\n",
    "               epochs = max_epochs) \n",
    "    \n",
    "# Decrease the learning rate to prevent overfitting\n",
    "model.alpha -= 0.0002\n",
    "\n",
    "# Fix the learning rate by having no decay\n",
    "model.min_alpha = model.alpha\n",
    "\n",
    "# Save the model to use later \n",
    "model.save(\"d2v.pm.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0875206  -3.55627275  0.58215928 -0.11968964 -1.55315423  0.41843811\n",
      "  0.89680219  0.56439292 -0.64858818 -0.2657198 ]\n"
     ]
    }
   ],
   "source": [
    "# Make the text into features found in training and test sets\n",
    "# Note: Python indexes at 0, not 1\n",
    "\n",
    "# np.zeros: Return a new array of given shape and type, filled with zeros\n",
    "train_text_set = np.zeros((training_size, text_input_dimensions))\n",
    "test_text_set = np.zeros((testing_size, text_input_dimensions))\n",
    "\n",
    "# Match up the trained vectors to the training set \n",
    "i = 0\n",
    "for i in range(training_size):\n",
    "    train_text_set[i] = model.docvecs['Text_'+str(i)] # .docvecs: Holds all trained vectors for the 'document tags' seen during training\n",
    "\n",
    "# Match up \n",
    "j = 0\n",
    "for i in range(training_size, training_size + testing_size):\n",
    "    test_text_set[j] = model.docvecs['Text_'+str(i)]\n",
    "    j=+1\n",
    "\n",
    "# Print results \n",
    "print(test_text_set[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizing genes and variants \n",
    "    # Using singular-value decomp. (SVD) to have more manageable features\n",
    "    \n",
    "# Readings:\n",
    "    # SVD: https://medium.com/the-andela-way/foundations-of-machine-learning-singular-value-decomposition-svd-162ac796c27d\n",
    "    # get_dummies function: http://fastml.com/how-to-use-pd-dot-get-dummies-with-the-test-set/\n",
    "        # get_dummies(): Convert categorical variable into dummy/indicator variables\n",
    "    # \"One Hot\" Encoding: https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "        # Seems to be better than label encoding so as not to rely on categorical placement/incorrect weights\n",
    "        # \"This is why we use one hot encoder to perform 'binarization' of the category and include it as a feature to train the model.\"\n",
    "    # Difference between LabelEncoder and pd.get_dummies(): https://stackoverflow.com/questions/38413579/what-is-the-difference-between-sklearn-labelencoder-and-pd-get-dummies\n",
    "        # LabelEncoder(): \"condenses\" the information by changing things to integers\n",
    "        # get_dummies(): \"expands\" the dimensions allowing (possibly) more convenient access\n",
    "\n",
    "# Set gene dimensions\n",
    "gene_input_dimensions = 25\n",
    "\n",
    "# Made the SVD model to truncate the gene and variant information later \n",
    "# SVD: Factorize into 3 matrices, then decompose into lower rank matrices without losing much of the important data\n",
    "svd = TruncatedSVD(n_components = 25,\n",
    "                   n_iter = gene_input_dimensions,\n",
    "                   random_state = 12)\n",
    "\n",
    "# Convert the categorical gene information by expanding it into different columns using one-hot encoding \n",
    "# Decompose with SVD \n",
    "one_hot_gene = pd.get_dummies(all_the_data[\"Gene\"])\n",
    "svd_one_hot_gene = svd.fit_transform(one_hot_gene.values)\n",
    "\n",
    "# Convert the categorical variant information by expanding it into different columns using one-hot encoding\n",
    "# Decompose with SVD \n",
    "one_hot_variant = pd.get_dummies(all_the_data[\"Variant\"])\n",
    "svd_one_hot_variant = svd.fit_transform(one_hot_variant.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Output the class encoding \n",
    "# LabelEncoder():\n",
    "    # Can be used to normalize labels\n",
    "    # Can be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels \n",
    "    \n",
    "# Consolidate the dataset using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the training set\n",
    "label_encoder.fit(train_y)\n",
    "\n",
    "# Convert the array of labeled data to one-hot encoded vector\n",
    "# Apply to pandas dataframe \n",
    "encoded_training_y = np_utils.to_categorical((label_encoder.transform(train_y)))\n",
    "print(encoded_training_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape is:  (3321, 350)\n",
      "Test set shape is:  (986, 350)\n",
      "\n",
      " Training set example rows:\n",
      "[-3.83415240e-23 -8.12746096e-19  3.25283397e-22 -3.52950754e-22\n",
      " -1.91577362e-22  3.54676519e-25  6.61614239e-25 -2.40019986e-27\n",
      "  3.49275715e-28  7.31181426e-30]\n",
      "\n",
      " Test set example rows:\n",
      "[ 2.69770557e-32 -2.77827596e-27 -3.63497563e-27 -3.12866459e-27\n",
      "  7.75549748e-27 -1.13244416e-26 -1.64947289e-26 -8.94629705e-26\n",
      " -3.75320953e-23 -1.05563464e-21]\n"
     ]
    }
   ],
   "source": [
    "# View consolidated training and testing sets\n",
    "# np.hstack(): Used to stack the sequence of input array horizontally (so, column-wise) to make a single array\n",
    "\n",
    "training_set = np.hstack((svd_one_hot_gene[:training_size], svd_one_hot_variant[:training_size], train_text_set))\n",
    "testing_set = np.hstack((svd_one_hot_gene[training_size:], svd_one_hot_variant[training_size:], test_text_set))\n",
    "\n",
    "# Dataset Attribute Updates \n",
    "print(\"Training set shape is: \", training_set.shape) \n",
    "print(\"Test set shape is: \", testing_set.shape) \n",
    "\n",
    "print(\"\\n Training set example rows:\")\n",
    "print(training_set[0][:10])\n",
    "\n",
    "print(\"\\n Test set example rows:\")\n",
    "print(testing_set[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.05.15-10:47:41\n"
     ]
    }
   ],
   "source": [
    "# Reading\n",
    "    # NN: https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "    # Train/Test Split: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    # NN Example: https://bit.ly/2WEY2Hs\n",
    "    # Simpler breakdown of components needed for NN: https://towardsdatascience.com/understanding-neural-networks-what-how-and-why-18ec703ebd31\n",
    "\n",
    "# Logistics \n",
    "now = datetime.now()\n",
    "time = now.strftime(\"%Y.%m.%d-%H:%M:%S\") # Formatting date and time \n",
    "print(time)\n",
    "directory_name = \"output/\"  # Output Directory\n",
    "filename = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (350, 2656)\n",
      "X_validation:  (350, 665)\n",
      "Y_train:  (9, 2656)\n",
      "Y_validation:  (9, 665)\n",
      "X_test:  (350, 986)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into the training and validation sets \n",
    "# Training set\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(training_set, encoded_training_y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "# Transpose the training set\n",
    "X_train, X_validation, Y_train, Y_validation = X_train.T, X_val.T, Y_train.T, Y_val.T\n",
    "\n",
    "# Transpose the testing set \n",
    "X_test = testing_set.T\n",
    "\n",
    "# View the dataset shapes to create placeholders later for TensorFlow...\n",
    "\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_validation: \", X_val.shape)\n",
    "print(\"Y_train: \", Y_train.shape)\n",
    "print(\"Y_validation: \", Y_val.shape)\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to initialize the parameters that TensorFlow will use to build a NN\n",
    "    # \"Tensor\" dictionaries containing weights and biases for each layer\n",
    "    # Keeping things simple(r), will only program 3 layers for now\n",
    "    \n",
    "def initialize_tf_parameters():\n",
    "    parameters = {}\n",
    "    tf.set_random_seed(1) # keep things the same across the entire NN \n",
    "        # Layer 1\n",
    "    weights_1 = tf.get_variable('weights_1', [350, X_train.shape[0]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    bias_1 = tf.get_variable('bias_1', [350, 1], initializer = tf.zeros_initializer())\n",
    "        # Layer 2\n",
    "    weights_2 = tf.get_variable('weights_2', [350, 350], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    bias_2 = tf.get_variable('bias_2', [350, 1], initializer = tf.zeros_initializer())\n",
    "        # Layer 3 \n",
    "    weights_3 = tf.get_variable('weights_3', [100, 350], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    bias_3 = tf.get_variable('bias_3', [100, 1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"weights_1\": weights_1,\n",
    "                  \"bias_1\": bias_1,\n",
    "                  \"weights_2\": weights_2,\n",
    "                  \"bias_2\": bias_2,\n",
    "                  \"weights_3\": weights_3,\n",
    "                  \"bias_3\": bias_3}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Create a function to create the placeholders needed for TensorFlow \n",
    "    # input_x: Dimensions of the input (scalar)\n",
    "    # input_y: # of classes (there are 9 mutational classes and Python indexes from 0, so you need 8 as the input here)\n",
    "    # X: Placeholder in floats with the shape of input_x \n",
    "    # Y: Placeholder in floats with the shape of input_y \n",
    "    \n",
    "def create_tf_placeholders(input_x, input_y):\n",
    "    X = tf.placeholder(tf.float32, shape = (input_x, None), name = \"X\") \n",
    "    Y = tf.placeholder(tf.float32, shape = (input_y, None), name = \"Y\")\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"forward propogation\" function for the NN\n",
    "## Reading\n",
    "    # Consider \"Activation Functions\": https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76 (???)\n",
    "    # Rectified Linear Unit (ReLU): https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/weights_\n",
    "    # Softmax: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\n",
    "        # Softmax assumes that each example is a member of exactly one class (which is what we need, each mutation will be sorted into 1 of 9 classes)\n",
    "    # Keeping probabilites: https://stats.stackexchange.com/questions/324914/value-of-the-keep-probability-when-calculating-loss-with-dropout\n",
    "        # Meant to calculate loss of dropping out things \n",
    "        # Will calculate loss later\n",
    "        \n",
    "# Input and output for forward-feeding function \n",
    "    # Combination of linear and ReLU to generate a softmax \n",
    "    # Takes in the placeholder generated in the function prior \n",
    "    # Takes in the parameters of weights and biases for each of the layers\n",
    "    # Keeps probablities for calculating the loss of dropouts \n",
    "    # Returns the linear transformation of the 3 layer NN (Z_linear_transform_3)\n",
    "    \n",
    "def forward_propagation(X, parameters, keep_probability_1, keep_probability_2):\n",
    "        # Parameters from dictionary of parameters \n",
    "    weights_1 = parameters['weights_1']\n",
    "    bias_1 = parameters['bias_1']\n",
    "    weights_2 = parameters['weights_2']\n",
    "    bias_2 = parameters['bias_2']\n",
    "    weights_3 = parameters['weights_3']\n",
    "    bias_3 = parameters['bias_3']\n",
    "        # Forward feeding the NN\n",
    "    Z_linear_transform_1 = tf.matmul(weights_1, X) + bias_1  # Z1 = np.dot(W1, X) + b1\n",
    "    A_post_activation_1 = tf.nn.relu(Z_linear_transform_1)  # A1 = relu(Z1)\n",
    "    A_post_activation_1 = tf.nn.dropout(A_post_activation_1, keep_probability_1)  # add dropout\n",
    "    Z_linear_transform_2 = tf.matmul(weights_2, A_post_activation_1) + bias_2  # Z2 = np.dot(W2, a1) + b2\n",
    "    A_post_activation_2 = tf.nn.relu(Z_linear_transform_2)  # A2 = relu(Z2)\n",
    "    A_post_activation_2 = tf.nn.dropout(A_post_activation_2, keep_probability_2)  # add dropout\n",
    "    Z_linear_transform_3 = tf.matmul(weights_3, A_post_activation_2) + bias_3  # Z3 = np.dot(W3,Z2) + b3\n",
    "    A_post_activation_3 = tf.nn.relu(Z_linear_transform_3)\n",
    "\n",
    "    return Z_linear_transform_3\n",
    "\n",
    "# Create a function that will calculate loss\n",
    "    # Loss functions: https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "    # \"... must faithfully distill all aspects of the model down into a single number in such a way that improvements in that number are a sign of a better model\"\n",
    "    # TensorFlow will do this for you: https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2\n",
    "        #\n",
    "# Input and output for the cost function \n",
    "    # Z_linear_transform of the last linear unit given from the forward propogation\n",
    "    # Placeholder that is the same shape of the last linear unit \n",
    "    # Returns TensorFlow function of cost \n",
    "\n",
    "def calculate_cost(Z_linear_transform_3, Y):\n",
    "    # TensforFlow has a function that will do this for you\n",
    "    # You just need to clean up the data a little bit by transposing it \n",
    "    logits = tf.transpose(Z_linear_transform_3) # Unscaled log probabilities\n",
    "    labels = tf.transpose(Y) # Each vector along the class dimension should hold a valid probability distribution\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: mini batches? predict function. back propogation? Running the model.\n",
    "# TODO: Formatting and submission\n",
    "# TODO: Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
